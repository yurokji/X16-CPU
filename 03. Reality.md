CHAPTER THREE

Reality

Capers of the Unseen Sun

“Evolutionarily speaking, visual perception is useful only if it is reasonably accurate. Indeed, vision is useful precisely because it is so accurate. By and large, what you see is what you get. When this is true, we have what is called veridical perception perception that is consistent with the actual state of affairs in the environment. This is almost always the case with vision.”
—STEPHEN PALMER , VISION SCIENCE

“I don’t see why you pick on neurons,” Francis Crick wrote on April 13, 1994. “Surely you believe the sun existed before there was anyone to perceive it. So why should neurons be different?” A few weeks earlier, Crick had kindly sent me a signed copy of his new book The Astonishing Hypothesis. I read it and sent him a letter, on March 22, thanking him for the book. I also raised a question about its hypothesis:

Perhaps you could help me escape what seems a paradox. I agree wholeheartedly with you that “seeing is an active, constructive process,” that what we see “is a symbolic interpretation of the world,” and that “in fact we have no direct knowledge of objects in the world.” Indeed I think perception to be like science: a process of constructing theories given the available evidence. We see the theories we believe. As you say, “seeing is believing.”

On these points, Crick and I agreed. But they conflict with common sense, and so they warrant some discussion. Most of us don’t claim to know exactly how seeing works. But if pressed, we may speculate that it’s much like a video camera. There is, we believe, a real 3D world that exists even when no one looks, and it contains real objects such as red apples and misty waterfalls. When we look, we simply shoot a video of this world. There’s really not much to it, and most of the time it works quite well—our video shots are accurate.

But common sense is in for a surprise. Neuroscientists assure us that each time we open our eyes, billions of neurons and trillions of synapses spring into action. Roughly one-third of the brain’s cortex, one-third of our most advanced computing power, is engaged in vision—which is not what you may expect if seeing is just a matter of shooting videos. Cameras, after all, were filming long before the era of the computer. So what in the world is the brain computing when we look, and why?

The standard reply by neuroscientists is that the brain is constructing, in real time, our perceptions of objects such as apples and waterfalls. 1 It constructs them because the eye itself does not see apples and waterfalls; instead, it has about 130 million photoreceptors, and each of them sees just one thing: how many photons of light it just captured. So the photoreceptors are bean counters for photons, and issue boring reports, something like this: Photoreceptor #1: twenty photons; Photoreceptor #2: three photons; Photoreceptor #130,000,000: six photons. There are, at the photoreceptors of the eye, no luscious apples and no dazzling waterfalls. There is just a stupefying array of numbers, with no obvious meaning. To endow this hill of beans with meaning, to understand what these lifeless numbers say about a living world, is such a daunting task that billions of neurons, including many millions within the eye itself, are conscripted into service. It’s not like translating Greek to English. It’s more like detective work: the numbers are cryptic clues, and the brain must sleuth like Sherlock. Or it’s like theoretical physics: the numbers are experimental data and the brain must pull an Einstein. With clever detective work and theorizing, your brain interprets a jumble of numbers as a coherent world, and that interpretation is what you see—the best theory your brain could muster.

Which is why Crick claimed, and I agreed, that “seeing is an active, constructive process,” that what we see “is a symbolic interpretation of the world,” that “in fact we have no direct knowledge of objects in the world,” and that seeing is believing your best theory.

But then I set up my paradox. If we construct everything we see, and if we see neurons, then we construct neurons. But what we construct doesn’t exist until we construct it (too bad; it would be much cheaper to move into my dream mansion before constructing it). So neurons don’t exist until we construct them.

But this conclusion, I wrote in that March 22 letter, “contradicts, it would seem, the astonishing hypothesis, viz., that neurons exist prior to and are, somehow, causally responsible for, our perceptions.”

I didn’t expect that Crick would buy my argument. But I was interested to hear why. He wrote back on March 25, 1994: “It is a reasonable hypothesis that a real world consists of which we only have limited knowledge and that neurons existed prior to anyone observing them as neurons .” (The emphasis is by Crick, which he indicated by underlining.)
Crick argued, and most neuroscientists would agree, that it’s reasonable to assume that neurons exist prior to anyone perceiving them as neurons. But I wanted to better understand his thoughts on the relation between perception and reality. So in a letter on April 11, 1994, I pressed further. “We can, as you say, hypothesize , that neurons exist in the world prior to any representations of them. But this hypothesis, though reasonable, is untestable. How shall we, in principle, falsify it?”

This prompted Crick to reply on April 13: “I don’t see why you pick on neurons. Surely you believe the sun existed before there was anyone to perceive it. So why should neurons be different?” But then, as I had hoped, he shared his thoughts on perception and reality. “It seems to me, following Kant, one has to distinguish between the thing-in-itself (the sun in the above example), which is essentially unknowable, and the ‘idea-of-the-thing,’ which is what our brains construct. Then the argument becomes what are perceived are symbolic constructions. The sun-in-itself can be the subject of perception. Our idea-of-the-sun is a symbolic construction. The idea-of-the-sun does not exist prior to its construction—but the sun-in-itself did!”

Fair enough. Crick rejected, and so do I, metaphysical solipsism, which says that I and my experience are all that exist. According to this solipsism, if I see you then you exist, but only as my experience. When I close my eyes, you cease to exist. I reside in a universe of my own making, a universe of my experiences. I am alone. I cannot join a Society of Solipsists or wonder, without irony, why more people aren’t solipsists.

Crick embraced metaphysical realism. The sun-in-itself exists even when no one looks. I only construct my perception of that sun—my idea-of-the-sun.

Most of us are metaphysical realists. It seems to be a view that comes naturally. Suppose, as we discussed in chapter one, that you open your eyes and have an experience that you describe as a red tomato a meter away. Then you close your eyes and your experience changes, to a gray field. Is it still true, while you see gray, that there is a red tomato a meter away? Most of us would say yes. Now this tomato that we believe exists, even when no one looks, is what Crick would call the “tomato-in-itself.” It is not the same as your experience of a tomato (or, as philosophers helpfully put it, “your experience as of a tomato”), your “idea-of the-tomato.”

Crick said in his letter that the thing-in-itself—the tomato-in-itself or the neuron-in-itself—“is essentially unknowable.” But most of us believe otherwise. We believe, for instance, that the tomato-in-itself is, like our experience, red and tomato-shaped and a meter away. We believe that experience accurately depicts the thing-in-itself.

I suspected that Crick also believed this. He believed that our idea-of-the-neuron accurately depicts the neuron-in-itself. The 3D shape of a neuron that a neuroscientist experiences when she looks through a microscope tells her the true shape of a neuron-in-itself. The clicks she hears from a microelectrode tell her the true activity of a neuron-in-itself. In his book, Crick said, “The Astonishing Hypothesis is that ‘You,’ your joys and your sorrows, your memories and your ambitions, your sense of personal identity and free will, are in fact no more than the behavior of a vast assembly of nerve cells and their associated molecules. ‘You’re nothing but a pack of neurons.’ ” Crick clearly meant a pack of neurons-in-themselves, not a pack of ideas-of-neurons.

So I wrote him another letter, on May 2, 1994, asking for his thoughts about this central issue.

“The Astonishing Hypothesis is still untestable. For only the idea-of-neuron is observed in experiments, not the neuron-in-itself. And the only way to bridge this gap, so far as I can see, is to hypothesize that the neuron-in-itself is, in important ways, similar to our idea-of-neuron. (These remarks, if correct, hold also for the sun-in-itself and so on.) Let’s call this the Bridge Hypothesis )

“In short, I think the Astonishing Hypothesis, even in its revised form, is untestable. Or rather, it is testable only if one assumes the Bridge Hypothesis which, since it asserts a relationship between the perceived and the unperceivable, is itself untestable and dubious. The thing-in-itself is ontological baggage, not useful for the scientific enterprise.”

I didn’t buy the bit about baggage, and I figured Crick wouldn’t buy it either, but I wanted to hear his thoughts.

Crick responded on May 4, 1994: “I don’t think it sensible to discard the “thing-in-itself,” as the idea is of some use in warning us about what we cannot know. It is, however, a hypothesis that we can usefully talk in this way, but it is the standard hypothesis underlying all science, even (I think) quantum mechanics. The problem only becomes acute when we discuss qualia.”

The term qualia is sometimes used by philosophers to refer to subjective, conscious experiences—what it’s like to see the redness of red or smell the aroma of coffee. I will avoid this term because it often triggers debates about its precise definition. I will instead refer to conscious experiences.

Crick continued. “In fact, our present tentative view of the way the brain works would suggest that some aspects of qualia cannot be communicated. The problem, rather, is to explain why qualia exist at all. The party line is that we should try to find out the NCC (the Neural Correlate of Consciousness) before we worry too much about this aspect of qualia.”

Crick was pragmatic about the thing-in-itself: it is a hypothesis that we can usefully talk this way (he underlined “hypothesis” and “usefully”). He was frank about the problem of conscious experiences. Their very existence was, he thought, too hard to explain at the time. In his quest to understand DNA, Crick was famously influenced by Schrodinger’s thoughts about genes in the book What is Life? Apparently, Crick was also influenced by Schrodinger’s thoughts, in that same book, about conscious experiences: “The sensation of color cannot be accounted for by the physicist’s objective picture of light-waves. Could the physiologist account for it, if he had fuller knowledge than he has of the processes in the retina and the nervous processes set up by them in the optical nerve bundles and in the brain? I do not think so.”

Crick assumed, however, that the thing-in-itself can be described using the vocabulary of our ideas-of-things, of objects moving in space and time. Heat-in-itself, for instance, is molecular motion in space and time; a neuron-in-itself is an object with a shape and activity that evolves in space and time. He assumed that our ideas-of-things truly describe the thing-in-itself, so that the same vocabulary describes both. I rejected this assumption as implausible. But Crick thought it applied even to objects, space, and time.

Crick was supported in his view by a young neuroscientist, David Marr, who revolutionized our understanding of vision in the late 1970s and early 1980s. Crick met Marr in England. Crick then moved to the Salk Institute in San Diego, and Marr moved to MIT. In April of 1979, Marr and his colleague Tomaso Poggio spent a month with Crick at the Salk, discussing visual neuroscience.

Marr claimed that our perceptions normally match reality, that our ideas-of-things correctly describe the things-in-themselves. As he put it in his 1982 book Vision : “usually our perceptual processing does run correctly (it delivers a true description of what is there).” He believed that this match between perception and reality was the result of a long process of evolution: “We very definitely do compute explicit properties of the real visible surfaces out there, and one interesting aspect of the evolution of visual systems is the gradual movement toward the difficult task of representing progressively more objective aspects of the visual world.”

The human visual system, Marr argued, evolved its ideas-of-things to match the true structure of the things-in-themselves, although the match is not always perfect: “usually our perceptual processing does run correctly (it delivers a true description of what is there), but although evolution has seen to it that our processing allows for many changes (like inconstant illumination), the perturbation due to the refraction of light by water is not one of them.” But Marr concluded that natural selection had, on balance, shaped our perceptions to match reality: “The payoff is more flexibility; the price, the complexity of the analysis and hence the time and size of brain required for it.”

Crick argued that the thing-in-itself is a useful hypothesis. Marr argued further, on evolutionary grounds, that our perceptions, our ideas-of-things, depict reality, the thing-in-itself, with accuracy. In my 1994 exchange with Crick, I had no counter to Marr’s argument from evolution for the Bridge Hypothesis.

Indeed, my thoughts on perception and reality were shaped by Marr. I first encountered his ideas in a graduate class on Artificial Intelligence at UCLA in the 1977–78 academic year. I was a senior, working toward a Bachelor of Arts in Quantitative Psychology, but Professor Edward Carterette kindly allowed me into his graduate class. One paper we discussed was by Marr. I found it electrifying in style and content. Marr built models of vision that were precise enough to be programmed into a computer. If the computer was then linked to video cameras, these programs could analyze the images received from the cameras, and infer important features of the nearby environment, such as its 3D structure. Marr’s goal was clear: create precise models of human vision and use them to build computers and robots that see.

I was hooked. Where was this guy, and how could I work with him? I was surprised to learn that Marr was in the Psychology Department at MIT. Psychology at MIT? I thought of MIT as a bastion of math and hard science, not psychology. I later learned that Marr was also in the Artificial Intelligence Laboratory. I decided to apply to MIT to be his student. The Cold War was at full fever, and I worked my way through UCLA as a cold warrior, employed by Hughes Aircraft to write flight simulators and cockpit displays for fighter jets, such as the F-14, in the machine code of a microprocessor called the AN/UYK-30. I graduated from UCLA in June of 1978, continued at Hughes for another year, and entered MIT in the fall of 1979 as Marr’s graduate student.

I soon learned that Marr had leukemia. He died fourteen months later, in November of 1980, at the age of thirty-five. But those fourteen months exceeded my expectations. Marr inspired in person as he did in print. He was the center of gravity for a community of eager students and brilliant colleagues. Discussions were lively, multidisciplinary, and game-changing.

There were ups. Marr went into remission and married Lucia Vaina. There were downs. Jeremy, a grad student in psychology, completed his PhD that spring and the next day took his life—the rumor was cyanide. All of the grad students were dazed. Days later, as I walked by Marr’s office on the eighth floor of the Artificial Intelligence Lab, he waved me in. “If you ever feel like ending your life, come see me first. Life is worth living.”

Marr soon came to lab meetings visibly weakened, with a handkerchief over his nose and mouth. Then, tragically, not at all. Whitman Richards, a brilliant psychophysicist and advocate of Marr’s ideas, was my coadvisor while Marr was alive, became my sole advisor after his death, and remained a dear friend until his own death in 2016.

I completed my PhD in the spring of 1983, and in the fall joined the Department of Cognitive Sciences at UC Irvine. By 1986, I doubted Marr’s claim that we evolved “to see a true description of what is there.” I also doubted that the language of our perceptions—the language of space, time, shapes, colors, textures, smells, tastes, and so on—can frame a true description of what is there. It is simply the wrong language. But I was unable, in 1994, to offer Crick a good argument against Marr’s claim.

Indeed, there is, to the contrary, a stock argument in its favor: those of our predecessors who saw reality more accurately had a competitive advantage over those who saw it less accurately. They were more likely to pass on their genes that coded for more accurate perceptions. We are the offspring of those who, in each generation, saw more accurately. So we can be confident that, after thousands of such generations, we see reality as it is. Not all of reality, of course. Just the parts that matter for survival in our niche. As Bill Geisler and Randy Diehl put it: “In general, (perceptual) estimates that are nearer the truth have greater utility than those that are wide of the mark.” 2 Thus, “In general, it is true that much of human perception is veridical [accurate] under natural conditions.” 3

The evolutionary theorist Robert Trivers, whose insights into evolution transformed our understanding of social relations, makes a similar argument. “Our sense organs have evolved to give us a marvelously detailed and accurate view of the outside world our sensory systems are organized to give us a detailed and accurate view of reality, exactly as we would expect if truth about the outside world helps us to navigate it more effectively.” 4

Vision scientists disagree on many technical issues, such as the role of action and embodiment in perception, and whether perception involves construction, inferences, computations, and internal representations. But they do agree on this: the language of our perceptions is suitable to describe what exists when no one looks; and, in the normal case, our perceptions get it right.

For instance, in his textbook Vision Science , Stephen Palmer tells students of perception that “Evolutionarily speaking, visual perception is useful only if it is reasonably accurate.” The idea is that perceptions that are truer, that better match the state of the objective world, are thereby fitter. So natural selection shapes our perceptions to be truer.

Most perceptual theorists propose that the brain creates internal representations of the outside world, and that these internal representations are responsible for our perceptual experiences. They claim that our experiences are veridical, meaning that the structure of these internal representations, and therefore of our experiences, matches the structure of the objective world.

Alva Noë and Kevin O’Regan tell us, “Perceivers are right to take themselves to have access to environmental detail.” 5 Noë and O’Regan agree that the brain creates internal representations of the outside world, but claim that these internal representations are not responsible for our experiences. They propose instead that our perceptual experiences arise from our active exploration of the objective world, and our discovery, in this process, of contingencies between our actions and perceptions. But they agree that this process results in perceptual experiences that are veridical.

Zygmunt Pizlo and his colleagues tell us, “veridicality is an essential characteristic of perception and cognition. It is absolutely essential. Perception and cognition without veridicality would be like physics without the conservation laws .” 6 The emphasis is theirs. Pizlo argues that our perceptions are veridical because evolution has shaped our sensory systems to perceive real symmetries in the outside world.

Some researchers, such as Jack Loomis, agree that there are similarities between our perceptions and objective reality, but contend that our perceptions can have systematic errors, especially of perceived shape. 7 These researchers assume, however, that the language of our perceptions is the right language to frame true descriptions of what is there.

But despite the consensus of experts, I doubted that natural selection favors perceptions that describe reality. More deeply, I doubted that selection favors perceptions that could even frame true descriptions of reality. It’s not that on occasion a perception exaggerates, underestimates, or otherwise goes awry, it’s that the lexicon of our perceptions, including space, time, and objects, is powerless to describe reality.

I found an argument for doubt from Marr himself, in his book Vision , an argument he aimed at simpler organisms, such as flies and frogs. “Visual systems like the fly’s are not very complicated; very little objective information about the world is obtained. The information is all very subjective.” He argued that “it is extremely unlikely that the fly has any explicit representation of the visual world around him—no true conception of a surface, for example.” But he thought that, despite its failure to represent the world, the fly could still survive because it can, for instance, “chase its mate with sufficiently frequent success.” 8

Then Marr explained how a simple system that “does not really represent the visual world about it” may nevertheless evolve. “One reason for this simplicity must be that these facts provide the fly with sufficient information for it to survive.” 9

Marr argued that natural selection can favor simple, subjective perceptions, that don’t represent objective reality, if they do guide adaptive action. This raises the question: When does natural selection favor veridical perceptions over subjective perceptions? Marr answered: when organisms get more complex. Humans, he claimed, have veridical perceptions, and simple flies do not. But is this correct?

Perhaps not. The cognitive scientist Steven Pinker has explained why natural selection may not favor veridical perceptions. My last year at MIT as a graduate student was Pinker’s first year there as an assistant professor. I had the pleasure of taking one of his classes and becoming dear friends. It was obvious then that, with his creativity, incisive logic, and encyclopedic mastery of the literature, he would make stellar contributions to the cognitive sciences, as in fact he has. His 1997 book, How the Mind Works, focused my attention on evolutionary psychology. 10 Before I read his book, I knew about evolutionary psychology and the groundbreaking work of Leda Cosmides and John Tooby. Indeed, I had tried and failed to persuade my department, in 1991, to offer Leda a faculty position—evolutionary psychology was, and still is, controversial. It has been accused, for instance, of lacking hypotheses that are testable, justifying unsavory moral and political ideas, and claiming that human behavior is determined by genes, with little influence from the environment. These accusations are misguided.

Pinker’s book persuaded me to study perception as a product of natural selection. He makes a surprising claim: “Our minds evolved by natural selection to solve problems that were life-and-death matters to our ancestors, not to commune with correctness.” This observation is central. Our minds were shaped by natural selection to solve life-and-death problems. Full stop. They were not shaped to commune with correctness. Whether our beliefs and perceptions happen to be true is a question that requires careful study.
In his critique of How the Mind Works , Jerry Fodor argued that no such study is needed, because nothing in science “shows, or even suggests, that the proper function of cognition is other than the fixation of true beliefs.” 11

In reply, Pinker offered several reasons why beliefs may evolve to be false. 12 For instance, computing the truth is costly in time and energy, and so we often use heuristics that risk being false or out of date. Pinker conceded, however, that “We do have some reliable notions about the distribution of middle-sized objects around us.” 13
What about those middle-sized objects around us—tables, trees, and tomatoes? When we see them, it feels like we see the truth. Most vision scientists concur: if I see a tomato and then close my eyes, the tomato is still there.

But could we be wrong? Is it possible that there is no tomato if no one looks? No space and time? No neurons? No neural activity to cause, or be, our conscious experiences? Is it possible that we do not see reality as it is?

Stephen Hawking and Leonard Mlodinow argue for a model-dependent realism: “According to model-dependent realism, it is pointless to ask whether a model is real, only whether it agrees with observation. If there are two models that both agree with observation then one cannot say that one is more real than another.” 14

Hawking and Mlodinow then ask: “How do I know that a table still exists if I go out of the room and can’t see it?. One could have a model in which the table disappears when I leave the room and reappears in the same position when I come back, but that would be awkward. The model in which the table stays put is much simpler and agrees with observation.” 15

Indeed, if two models agree with observation, then prefer the simpler. But the model in which the neuron stays put has so far, and despite valiant efforts by talented neuroscientists, failed to explain the origin, nature, and data of conscious experience: no theory that starts with neurons and neural activity can account for observations about conscious experiences and their correlations with neural activity. Perhaps the model in which the neuron stays put is an impediment to our progress in understanding the origin of consciousness.

Philosophers have, for centuries, debated the puzzle of perception and reality. Can we transform this philosophical puzzle into a precise scientific question? Can Darwin’s theory of natural selection provide a definitive answer?

In 2007, I decided to try. It was time to see if neurons stay put, or if we should pick on them.
